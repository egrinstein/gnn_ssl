training:
  batch_size: 128
  n_epochs: 100
  learning_rate: 0.001
  learning_rate_decay_steps: [1, 3, 20]
  learning_rate_decay_values: 0.5
  n_workers: 1
  multi_gpu: True
  multi_gpu_strategy: ddp
  check_val_every_n_epoch: 1
  log_step: 50

  optimizer: adam
  betas: [0.9, 0.999]

  progress_bar_type: tqdm
  progress_bar_refresh_rate: 1
  # Early_stopping
  early_stopping:
    enabled: False
    key_to_monitor: validation_loss
    min_delta: 0.001
    patience_in_epochs: 10

  transfer_learning: false # Train sequentially using multiple datasets